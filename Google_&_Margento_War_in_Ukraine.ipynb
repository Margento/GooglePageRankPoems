{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Margento\n",
    "\n",
    "Google & Margento--War in Ukraine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "google search: ukraine war poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#The OS module in Python provides a way of using operating system dependent functionality. \n",
    "#The functions that the OS module provides allows you to interface with the underlying operating system \n",
    "#that Python is running on – be that Windows, Mac or Linux.\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import nltk\n",
    "#import logging\n",
    "#Module that records events related to the application’s operation. \n",
    "#The log record, which is created with every logging event, contains readily available diagnostic information such as \n",
    "#the file name, full path, function, and line number of the logging event.\n",
    "\n",
    "#from collections import Counter\n",
    "#from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#sklearn implements support vector classification; it is part of \n",
    "#scikit-learn, a free software machine learning library for Python (tools for data mining and data analysis)\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt  \n",
    "import numpy as np\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    #level=logging.INFO)\n",
    "#nltk.download('popular', halt_on_error=False)\n",
    "\n",
    "import re\n",
    "import codecs\n",
    "import string\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/christanasescu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('stop_words_poetry.txt')\n",
    "\n",
    "stopwords.append('...')\n",
    "stopwords.append(\"'d\")\n",
    "stopwords.append('...')\n",
    "stopwords.append(\"&\")\n",
    "stopwords.append(\"upon\")\n",
    "stopwords.append(\"also\")\n",
    "stopwords.append(\"hath\")\n",
    "stopwords.append(\"must\")\n",
    "stopwords.append(\"therefore\")\n",
    "stopwords.append(\"doth\")\n",
    "stopwords.append(\"could\")\n",
    "stopwords.append(\"would\")\n",
    "#stopwords.append(\"another\")\n",
    "stopwords.append(\"much\")\n",
    "#stopwords.append(\"give\")\n",
    "stopwords.append(\"like\")\n",
    "stopwords.append(\"since\")\n",
    "#stopwords.append(\"many\")\n",
    "#stopwords.append(\"without\")\n",
    "#stopwords.append(\"first\")\n",
    "stopwords.append(\"though\")\n",
    "#stopwords.append(\"well\")\n",
    "#stopwords.append(\"often\")\n",
    "#stopwords.append(\"great\")\n",
    "stopwords.append(\"either\")\n",
    "#stopwords.append(\"even\")\n",
    "stopwords.append(\"shall\")\n",
    "#stopwords.append(\"they\")\n",
    "stopwords.append(\"what\")\n",
    "stopwords.append(\"their\")\n",
    "#stopwords.append(\"more\")\n",
    "#stopwords.append(\"there\")\n",
    "#stopwords.append(\"your\")\n",
    "#stopwords.append(\"them\")\n",
    "stopwords.append(\"’\")\n",
    "stopwords.append(\"“\")\n",
    "stopwords.append(\"2\")\n",
    "stopwords.append(\"3\")\n",
    "stopwords.append(\"”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pre_clean(list_of_text):\n",
    "        '''\n",
    "        preliminary cleaning of the text\n",
    "        - remove new line character i.e. \\n or \\r\n",
    "        - remove tabs i.e. \\t\n",
    "        - remove extra spaces\n",
    "        '''\n",
    "        cleaned_list = []\n",
    "        for text in list_of_text:\n",
    "            # print(\"original:\", text)\n",
    "            text = text.replace('\\\\n', ' ')\n",
    "            text = text.replace('\\\\r', ' ')\n",
    "            text = text.replace('\\\\t', ' ')\n",
    "            pattern = re.compile(r'\\s+')\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "            text = text.strip()\n",
    "            # check for empty strings\n",
    "            if text != '' and text is not None:\n",
    "                cleaned_list.append(text)\n",
    "\n",
    "        return cleaned_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = _pre_clean(tokens)\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    #tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '00_annalyne_poem_to_putin.txt', 1: '01_russian_poet_arrested.txt', 2: '02_new_yorker_news.txt', 3: '03_celebrities_go_full_cringe.txt', 4: '04_ukraine_feature_pio.txt', 5: '05_ukrainian_lit.txt', 6: '06_olivia_wilde.txt', 7: '07_kobzar.txt', 8: '08_lesson_of_the_day.txt', 9: '09_ukrainian_needs_prayers.txt', 10: '10_transreading_russia.txt', 11: '11_ukraine_tls.txt', 12: '12_calamity_again.txt', 13: '13_war_advertisement.txt', 14: '14_putin_on_history_literature.txt', 15: '15_fakes_about_the_invasion.txt', 16: '16_poetry_of_war.txt', 17: '17_maps_&_poems.txt', 18: '18_massacre.txt', 19: '19_myroslav_laiuk.txt', 20: '20_ukrainian_lit_in_en.txt', 21: '21_songs.txt', 22: '22_special_operation.txt', 23: '23_jewish_poets.txt', 24: '24_holodomor.txt', 25: '25_surrender?.txt', 26: '26_poet_charged.txt', 27: '27_love_in_kyiv.txt', 28: '28_unlawful.txt', 29: '29_musk_tweets_poems.txt', 30: '30_bell_hooks.txt', 31: '31_tracy_k_smith.txt', 32: '32_poem_hunter.txt', 33: '33_twitter_poem.txt', 34: '34_catherine_the_great.txt', 35: '35_immigration_director_poem.txt', 36: 'antiwar_demo_margento.txt'}\n"
     ]
    }
   ],
   "source": [
    "HOME = os.getcwd()\n",
    "\n",
    "TEXTS_DIR = HOME + \"/fastText_multilingual-master/war_in_ukraine/\"\n",
    "\n",
    "#TEXTS_DIR = HOME\n",
    "\n",
    "filelabels = {}\n",
    "\n",
    "texts_data = []\n",
    "\n",
    "files = [f for f in os.listdir(TEXTS_DIR) if os.path.isfile(os.path.join(TEXTS_DIR, f))]\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "remove_punct_map = dict.fromkeys(map(ord, string.punctuation))\n",
    "\n",
    "tokens_total = []\n",
    "\n",
    "count = -1\n",
    " \n",
    "os.chdir(TEXTS_DIR)\n",
    "    \n",
    "for f in files:\n",
    "    #os.chdir(TEXTS_DIR)\n",
    "    with open(f, \"r\", encoding='utf-8', errors = 'ignore') as openf:\n",
    "        tokens = []\n",
    "        count = count + 1\n",
    "        filelabels[count] = os.path.basename(openf.name)\n",
    "        for line in openf:\n",
    "            sent_text = nltk.sent_tokenize(line)\n",
    "            for sentence in sent_text:\n",
    "                tokens1 = tokenize(sentence)\n",
    "                tokens1 = [item.translate(remove_punct_map)\n",
    "                      for item in tokens1]\n",
    "                #filter_object = filter(lambda x: x != \"\", tokens1)\n",
    "                tokens1 = [x for x in tokens1 if x!= \"\"]\n",
    "                tokens1 = [x.lower() for x in tokens1]\n",
    "                for token in tokens1:\n",
    "                    tokens.append(token)\n",
    "                    tokens_total.append(token)\n",
    "                #if random.random() > .99:\n",
    "                #print(tokens)\n",
    "    #print(tokens_total)\n",
    "    texts_data.append(tokens)\n",
    "\n",
    "print(filelabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the poems in the computationally assembled anthology MARGENTO (eds.) \"US\" Poets Foreign Poets\n",
    "# https://www.asymptotejournal.com/blog/2019/01/24/us-poets-foreign-poets-a-computationally-assembled-anthology/ \n",
    "# https://www.asymptotejournal.com/blog/2019/05/30/our-shared-world-of-language-reflections-on-us-poets-foreign-poets/\n",
    "# https://www.academia.edu/41440906/_US_Poets_Foreign_Poets_A_Computationally_Assembled_Anthology_MARGENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.extend(['a', 'like', 'you', 'they', 'he', 'be', 'it', 'your', 'her', 'of', 'more', 'there', 'no', 'not', '’', 'what', 'my', 'his', 'she', 'to', 'our', 'me', 'we', 'in', 'can', 'us', 'an', 'if', 'do', 'this', '”', 'because', 'who', 'and', 'but', 'him'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_total = [x for x in tokens_total if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "15\n",
      "20\n",
      "12\n",
      "22\n",
      "21\n",
      "14\n",
      "21\n",
      "18\n",
      "24\n",
      "19\n",
      "21\n",
      "20\n",
      "23\n",
      "14\n",
      "23\n",
      "18\n",
      "21\n",
      "22\n",
      "15\n",
      "12\n",
      "24\n",
      "17\n",
      "11\n",
      "14\n",
      "17\n",
      "28\n",
      "20\n",
      "28\n",
      "17\n",
      "22\n",
      "19\n",
      "19\n",
      "11\n",
      "20\n",
      "15\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(filelabels)):\n",
    "    print(len(texts_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(filelabels)):\n",
    "    texts_data[i] = [x for x in texts_data[i] if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(filelabels)):\n",
    "    #print(len(texts_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Topic modeling does not make sense [which we should have expected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documents(path):\n",
    "    os.chdir(path)\n",
    "    files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    texts = []\n",
    "    count = -1\n",
    "    for f in files:\n",
    "        with codecs.open(f, \"r\", encoding='utf-8', errors = 'ignore') as openf:\n",
    "            count = count + 1\n",
    "            filelabels[count] = os.path.basename(openf.name)\n",
    "            splitted_lines = openf.read().splitlines()\n",
    "            splitted_lines = _pre_clean(splitted_lines)\n",
    "            texts.append(splitted_lines)\n",
    "    #print(filelabels)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = get_documents(TEXTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = []\n",
    "\n",
    "for document in documents:\n",
    "    new_document = \"\"\n",
    "    for string_ in document:\n",
    "        exclude = set(string.punctuation)\n",
    "        string_ = ''.join(ch for ch in string_ if ch not in exclude)\n",
    "        lower_string = string_.lower()\n",
    "        new_document = \" \".join([new_document, lower_string]) \n",
    "    new_documents.append(new_document)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(documents[36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(new_documents[36])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(documents[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(new_documents[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = CountVectorizer(new_documents, stop_words = stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(new_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 444)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X.toarray()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_idf_vect = TfidfVectorizer(stop_words = stopwords)\n",
    "\n",
    "tfidf = tf_idf_vect.fit_transform(new_documents)\n",
    "\n",
    "#print(type(tfidf))\n",
    "\n",
    "W = tfidf.toarray()\n",
    "\n",
    "#print(type(W))\n",
    "\n",
    "dt = [('correlation', float)]\n",
    "\n",
    "similarity_matrix = np.matrix((tfidf * tfidf.T).A, dtype=dt)\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.from_numpy_matrix(similarity_matrix)\n",
    "\n",
    "weights = [(G[tpl[0]][tpl[1]]['correlation']) for tpl in G.edges()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = [(x, x) for x in G.nodes()] \n",
    "\n",
    "G.remove_edges_from(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[(1.,), (1.,), (1.,), (1.,), (1.,), (1.,), (1.,), (1.,), (1.,),\n",
       "         (1.,), (1.,), (1.,), (1.,), (1.,), (1.,), (1.,), (1.,), (1.,),\n",
       "         (1.,), (1.,), (1.,), (1.,), (1.,), (1.,), (1.,), (1.,), (1.,),\n",
       "         (1.,), (1.,), (1.,), (1.,), (1.,), (1.,), (1.,), (1.,), (1.,),\n",
       "         (1.,)]], dtype=[('correlation', '<f8')])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "similarity_matrix[range(len(filelabels)), range(len(filelabels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_matrix[range(len(filelabels)), range(len(filelabels))] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 37)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SAM = np.zeros(shape=(2*len(filelabels), 2*len(filelabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(SAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim_0 = np.array(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0.        ,) (0.        ,) (0.        ,) ... (0.        ,)\n",
      "  (0.        ,) (0.        ,)]\n",
      " [(0.        ,) (0.        ,) (0.04679713,) ... (0.03881888,)\n",
      "  (0.05178745,) (0.        ,)]\n",
      " [(0.        ,) (0.04679713,) (0.        ,) ... (0.05674513,)\n",
      "  (0.04351378,) (0.        ,)]\n",
      " ...\n",
      " [(0.        ,) (0.03881888,) (0.05674513,) ... (0.        ,)\n",
      "  (0.02074594,) (0.        ,)]\n",
      " [(0.        ,) (0.05178745,) (0.04351378,) ... (0.02074594,)\n",
      "  (0.        ,) (0.        ,)]\n",
      " [(0.        ,) (0.        ,) (0.        ,) ... (0.        ,)\n",
      "  (0.        ,) (0.        ,)]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(sim_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 37)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74, 74)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_0[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_0[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_0[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(filelabels)):\n",
    "    for j in range(len(filelabels)):\n",
    "        SAM[i][j] = sim_0[i][j][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "SAM[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04679713450038443"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAM[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastVector:\n",
    "    \"\"\"\n",
    "    Minimal wrapper for fastvector embeddings.\n",
    "    ```\n",
    "    Usage:\n",
    "        $ model = FastVector(vector_file='/path/to/wiki.en.vec')\n",
    "        $ 'apple' in model\n",
    "        > TRUE\n",
    "        $ model['apple'].shape\n",
    "        > (300,)\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vector_file='', transform=None):\n",
    "        \"\"\"Read in word vectors in fasttext format\"\"\"\n",
    "        self.word2id = {}\n",
    "\n",
    "        # Captures word order, for export() and translate methods\n",
    "        self.id2word = []\n",
    "\n",
    "        print('reading word vectors from %s' % vector_file)\n",
    "        with open(vector_file, 'r') as f:\n",
    "            (self.n_words, self.n_dim) = \\\n",
    "                (int(x) for x in f.readline().rstrip('\\n').split(' '))\n",
    "            self.embed = np.zeros((self.n_words, self.n_dim))\n",
    "            for i, line in enumerate(f):\n",
    "                elems = line.rstrip('\\n').split(' ')\n",
    "                self.word2id[elems[0]] = i\n",
    "                self.embed[i] = elems[1:self.n_dim+1]\n",
    "                self.id2word.append(elems[0])\n",
    "        \n",
    "        # Used in translate_inverted_softmax()\n",
    "        self.softmax_denominators = None\n",
    "        \n",
    "        if transform is not None:\n",
    "            print('Applying transformation to embedding')\n",
    "            self.apply_transform(transform)\n",
    "\n",
    "    def apply_transform(self, transform):\n",
    "        \"\"\"\n",
    "        Apply the given transformation to the vector space\n",
    "        Right-multiplies given transform with embeddings E:\n",
    "            E = E * transform\n",
    "        Transform can either be a string with a filename to a\n",
    "        text file containing a ndarray (compat. with np.loadtxt)\n",
    "        or a numpy ndarray.\n",
    "        \"\"\"\n",
    "        transmat = np.loadtxt(transform) if isinstance(transform, str) else transform\n",
    "        self.embed = np.matmul(self.embed, transmat)\n",
    "\n",
    "    def export(self, outpath):\n",
    "        \"\"\"\n",
    "        Transforming a large matrix of WordVectors is expensive. \n",
    "        This method lets you write the transformed matrix back to a file for future use\n",
    "        :param The path to the output file to be written \n",
    "        \"\"\"\n",
    "        fout = open(outpath, \"w\")\n",
    "\n",
    "        # Header takes the guesswork out of loading by recording how many lines, vector dims\n",
    "        fout.write(str(self.n_words) + \" \" + str(self.n_dim) + \"\\n\")\n",
    "        for token in self.id2word:\n",
    "            vector_components = [\"%.6f\" % number for number in self[token]]\n",
    "            vector_as_string = \" \".join(vector_components)\n",
    "\n",
    "            out_line = token + \" \" + vector_as_string + \"\\n\"\n",
    "            fout.write(out_line)\n",
    "\n",
    "        fout.close()\n",
    "\n",
    "    def translate_nearest_neighbour(self, source_vector):\n",
    "        \"\"\"Obtain translation of source_vector using nearest neighbour retrieval\"\"\"\n",
    "        similarity_vector = np.matmul(FastVector.normalised(self.embed), source_vector)\n",
    "        target_id = np.argmax(similarity_vector)\n",
    "        return self.id2word[target_id]\n",
    "\n",
    "    def translate_inverted_softmax(self, source_vector, source_space, nsamples,\n",
    "                                   beta=10., batch_size=100, recalculate=True):\n",
    "        \"\"\"\n",
    "        Obtain translation of source_vector using sampled inverted softmax retrieval\n",
    "        with inverse temperature beta.\n",
    "        nsamples vectors are drawn from source_space in batches of batch_size\n",
    "        to calculate the inverted softmax denominators.\n",
    "        Denominators from previous call are reused if recalculate=False. This saves\n",
    "        time if multiple words are translated from the same source language.\n",
    "        \"\"\"\n",
    "        embed_normalised = FastVector.normalised(self.embed)\n",
    "        # calculate contributions to softmax denominators in batches\n",
    "        # to save memory\n",
    "        if self.softmax_denominators is None or recalculate is True:\n",
    "            self.softmax_denominators = np.zeros(self.embed.shape[0])\n",
    "            while nsamples > 0:\n",
    "                # get batch of randomly sampled vectors from source space\n",
    "                sample_vectors = source_space.get_samples(min(nsamples, batch_size))\n",
    "                # calculate cosine similarities between sampled vectors and\n",
    "                # all vectors in the target space\n",
    "                sample_similarities = \\\n",
    "                    np.matmul(embed_normalised,\n",
    "                              FastVector.normalised(sample_vectors).transpose())\n",
    "                # accumulate contribution to denominators\n",
    "                self.softmax_denominators \\\n",
    "                    += np.sum(np.exp(beta * sample_similarities), axis=1)\n",
    "                nsamples -= batch_size\n",
    "        # cosine similarities between source_vector and all target vectors\n",
    "        similarity_vector = np.matmul(embed_normalised,\n",
    "                                      source_vector/np.linalg.norm(source_vector))\n",
    "        # exponentiate and normalise with denominators to obtain inverted softmax\n",
    "        softmax_scores = np.exp(beta * similarity_vector) / \\\n",
    "                         self.softmax_denominators\n",
    "        # pick highest score as translation\n",
    "        target_id = np.argmax(softmax_scores)\n",
    "        return self.id2word[target_id]\n",
    "\n",
    "    def get_samples(self, nsamples):\n",
    "        \"\"\"Return a matrix of nsamples randomly sampled vectors from embed\"\"\"\n",
    "        sample_ids = np.random.choice(self.embed.shape[0], nsamples, replace=False)\n",
    "        return self.embed[sample_ids]\n",
    "\n",
    "    @classmethod\n",
    "    def normalised(cls, mat, axis=-1, order=2):\n",
    "        \"\"\"Utility function to normalise the rows of a numpy array.\"\"\"\n",
    "        norm = np.linalg.norm(\n",
    "            mat, axis=axis, ord=order, keepdims=True)\n",
    "        norm[norm == 0] = 1\n",
    "        return mat / norm\n",
    "    \n",
    "    @classmethod\n",
    "    def cosine_similarity(cls, vec_a, vec_b):\n",
    "        \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n",
    "        return np.dot(vec_a, vec_b) / \\\n",
    "            (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.word2id\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.embed[self.word2id[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        if source in source_dictionary and target in target_dictionary:\n",
    "            source_matrix.append(source_dictionary[source])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/christanasescu/fastText_multilingual-master\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cd /Users/christanasescu/fastText_multilingual-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from wiki.en.vec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "en_dictionary = FastVector(vector_file='wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_words = set(en_dictionary.word2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "en_words = list(en_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/christanasescu/fastText_multilingual-master/war_in_ukraine'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/christanasescu/fastText_multilingual-master\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "HOME = os.getcwd()\n",
    "\n",
    "TEXTS_DIR = HOME + \"/war_in_ukraine/\"\n",
    "#Poems_Dir = HOME\n",
    "\n",
    "os.chdir(TEXTS_DIR)\n",
    "lines = [i for i in range(len(filelabels))]\n",
    "\n",
    "files = [f for f in os.listdir(TEXTS_DIR) if os.path.isfile(os.path.join(TEXTS_DIR, f))]\n",
    "\n",
    "i = -1\n",
    "\n",
    "\n",
    "#for i in range(len(filelabels)):\n",
    "    #lines_1 = []\n",
    "    #with open (List_poem_itinerary[i][1], 'rt') as file:\n",
    "for f in files:\n",
    "    with open(f, \"r\", encoding='utf-8', errors = 'ignore') as openf:\n",
    "        lines_1 = []\n",
    "        i = i + 1\n",
    "        filelabels[count] = os.path.basename(openf.name)\n",
    "        for line in openf:\n",
    "        #for line in file:\n",
    "                line = line.replace('\\\\n', ' ')\n",
    "                line = line.replace('\\\\r', ' ')\n",
    "                line = line.replace('\\\\t', ' ')\n",
    "                pattern = re.compile(r'\\s+')\n",
    "                line = re.sub(pattern, ' ', line)\n",
    "                line = line.strip()\n",
    "                line = line.lower()\n",
    "                # check for empty strings\n",
    "                if line != '' and line is not None:\n",
    "                    lines_1.append(line)\n",
    "        lines[i] = lines_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['annalynne mccord has recited a poem to president putin',\n",
       " 'if i was your mother, the world would have been warm / so much laughter and joy and nothing would harm']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "poems_tokens = []\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    poems_tokens.append(lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(lines)):\n",
    "    for j in range(len(lines[i])):\n",
    "        tokens = []\n",
    "        tokens1 = tokenize(lines[i][j])\n",
    "        tokens1 = [item.translate(remove_punct_map)\n",
    "                      for item in tokens1]\n",
    "        tokens1 = [x for x in tokens1 if x!= \"\"]\n",
    "        for token in tokens1:\n",
    "                    tokens.append(token)\n",
    "        poems_tokens[i][j] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(poems_tokens)):\n",
    "        for j in range(len(poems_tokens[i])):\n",
    "            poems_tokens[i][j] = [x for x in poems_tokens[i][j] if x not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['antiwar', 'demo', 'cathedra', 'markt', 'frozen', 'crowd', 'online', 'ant'],\n",
       " ['antwerp',\n",
       "  'twerp',\n",
       "  'crawling',\n",
       "  'indoor',\n",
       "  'cafes',\n",
       "  'away',\n",
       "  'calculated',\n",
       "  'sun']]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "poems_tokens[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectors_of_lines = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(poems_tokens)):\n",
    "    vectors_of_lines.append(poems_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def l2_norm(x):\n",
    "   return np.sqrt(np.sum(x**2))\n",
    "\n",
    "def div_norm(x):\n",
    "   norm_value = l2_norm(x)\n",
    "   if norm_value > 0:\n",
    "       return x * ( 1.0 / norm_value)\n",
    "   else:\n",
    "       return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(poems_tokens)):\n",
    "    vectors_of_lines[i] = []\n",
    "    #if i % 2 == 0:\n",
    "    for j in range(len(poems_tokens[i])):\n",
    "            vect1 = []\n",
    "            for k in range(len(poems_tokens[i][j])):\n",
    "                    if poems_tokens[i][j][k] in en_words:\n",
    "                        vect1.append(div_norm(en_dictionary[poems_tokens[i][j][k]]))\n",
    "                    else:\n",
    "                        continue\n",
    "            if len(poems_tokens[i][j]) != 0:\n",
    "                vect0 = sum(vect1) / len(poems_tokens[i][j])\n",
    "            else:\n",
    "                vect0 = 0\n",
    "            vectors_of_lines[i].append((j, vect0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  array([-6.94155203e-04, -1.61414387e-02, -1.91115497e-02,  3.23625525e-02,\n",
       "         -1.50783822e-02,  1.42198745e-02,  2.56544786e-03, -1.87598274e-02,\n",
       "         -8.40242078e-03,  3.15030796e-02, -1.49848870e-02, -1.55002940e-03,\n",
       "         -3.36727376e-02, -1.00805578e-02,  7.76782880e-03, -2.60345033e-02,\n",
       "          1.94894255e-02,  2.96792092e-02,  4.27760069e-02,  6.64073312e-02,\n",
       "         -2.70787235e-02,  3.62550934e-02, -5.25584264e-02, -2.31899221e-02,\n",
       "          2.28988049e-02, -2.17851858e-02, -3.35879615e-02,  1.00521742e-02,\n",
       "          4.01690490e-02,  2.41565466e-02, -9.05675738e-03,  8.00670029e-02,\n",
       "         -4.73933558e-02,  1.14361067e-02,  1.09559305e-02, -1.26858285e-02,\n",
       "          3.60750935e-03, -6.07972741e-03,  7.48561115e-02, -1.08029345e-02,\n",
       "          4.20380425e-03,  8.74910938e-03,  1.01369872e-03,  3.52562248e-02,\n",
       "          1.08907284e-02, -1.43620030e-02,  4.14647146e-02,  4.97246538e-03,\n",
       "          4.88153062e-03, -7.68878862e-03,  2.23933845e-02, -3.80065547e-02,\n",
       "          1.44090029e-02,  9.22710491e-04, -5.30997986e-02,  1.69589742e-02,\n",
       "          2.01281155e-02,  3.09828350e-02, -1.84090152e-03,  1.47812455e-02,\n",
       "         -1.27442932e-02,  1.35425546e-02,  1.01261712e-01, -2.34387785e-02,\n",
       "          1.77209740e-03, -8.17331662e-03, -1.51581370e-02,  2.94540983e-02,\n",
       "         -1.17568717e-03,  7.80631519e-03, -6.55753759e-04, -1.24335577e-02,\n",
       "          4.13470064e-02,  1.11071527e-02, -3.00405618e-03,  5.53287812e-02,\n",
       "          5.41393874e-02,  7.72490925e-03,  1.69492442e-03, -2.62294817e-02,\n",
       "          3.39770220e-02,  3.35273170e-03, -1.95687766e-02,  3.13912239e-03,\n",
       "         -4.06506894e-02,  2.45025178e-02,  1.30742688e-03,  3.97249990e-02,\n",
       "          1.56261614e-02, -1.08917964e-03,  1.52109659e-02, -5.11014305e-02,\n",
       "          7.18589480e-02, -2.31796799e-03,  3.74799940e-02,  3.84265751e-02,\n",
       "          1.58555279e-06,  4.09973434e-03,  6.87398431e-03, -1.64093416e-02,\n",
       "          2.30605629e-02, -2.32042463e-02,  9.97915106e-03, -2.55891320e-02,\n",
       "         -5.20731444e-03,  3.40799234e-03, -3.94849760e-02,  3.08636532e-02,\n",
       "         -1.09509513e-02,  2.15887424e-02, -2.92648722e-02, -2.72281084e-02,\n",
       "         -9.28397142e-03, -3.27578573e-02,  3.55496158e-03, -2.67332781e-03,\n",
       "          3.12714776e-02, -4.81792600e-03, -7.22333363e-03,  2.34286704e-02,\n",
       "          3.94753790e-02,  3.86450963e-02, -2.90071721e-02,  4.75640216e-02,\n",
       "          1.24488219e-02, -6.14130262e-03, -1.26287247e-02,  2.01296757e-02,\n",
       "          1.99099430e-02,  4.79988004e-02, -1.27282500e-02,  1.93376611e-02,\n",
       "          1.65823210e-02, -4.22844625e-02,  7.10224972e-03, -3.53339303e-03,\n",
       "         -5.96430815e-02,  2.41325492e-02,  4.51813508e-02,  3.68863381e-02,\n",
       "         -9.59765053e-04,  6.51105780e-02,  7.88238489e-02,  1.94130920e-02,\n",
       "         -1.62473683e-02,  6.53388434e-02, -2.65696572e-02, -4.17748964e-02,\n",
       "          2.50158814e-02,  1.62745696e-02,  2.38465189e-02, -4.03241968e-02,\n",
       "         -4.54864327e-02,  2.41922243e-02,  6.07442979e-03,  1.24720648e-03,\n",
       "          6.71969159e-03, -3.19732781e-02,  3.00954939e-02, -1.31643799e-02,\n",
       "          8.17501301e-03,  4.74557695e-02, -5.78904998e-02, -6.70620153e-03,\n",
       "          4.56273324e-02,  4.92939204e-02, -4.34882763e-02, -1.48538904e-02,\n",
       "          2.51652748e-02, -1.21336984e-03, -3.26668368e-02, -4.25586628e-02,\n",
       "         -4.41918581e-02, -3.72854652e-02, -2.29259720e-02,  4.81806820e-02,\n",
       "         -1.26894019e-02,  2.37069339e-02,  6.18976128e-03, -8.88255074e-03,\n",
       "          1.62263238e-02, -1.90879847e-02, -1.12715030e-02,  1.75616068e-02,\n",
       "         -2.85476741e-02,  5.67281471e-02, -3.98669716e-02, -3.32409938e-02,\n",
       "          5.32119179e-02,  1.50008847e-02, -8.48077569e-03, -6.53511360e-02,\n",
       "          1.19582054e-02,  1.36559585e-02, -2.29508357e-02, -1.07957485e-02,\n",
       "         -3.47713112e-03,  4.48937522e-03, -5.29573170e-02, -8.77739948e-03,\n",
       "          5.86919032e-02,  5.49617973e-03,  9.41025592e-03, -4.84680846e-03,\n",
       "          4.25834810e-02, -4.00440863e-02, -5.20420236e-02,  4.48368311e-02,\n",
       "          3.16253449e-02,  1.00538247e-03,  1.21633543e-01,  6.09600625e-03,\n",
       "          7.13369541e-03, -5.49724013e-02, -9.11475429e-03,  1.43859492e-02,\n",
       "          3.49634133e-03, -6.36631798e-02, -4.00140399e-02,  2.23807202e-02,\n",
       "         -1.28538909e-02, -2.31799698e-02, -4.33925705e-02,  2.26707375e-02,\n",
       "         -4.68583169e-02, -1.38696646e-02, -2.57529842e-02, -5.59368614e-02,\n",
       "         -1.92350206e-02, -2.44752922e-02,  4.65300951e-02,  1.85841638e-02,\n",
       "         -2.31491452e-02,  9.98840137e-04, -1.40296629e-02,  1.59925679e-02,\n",
       "          2.47851151e-02,  3.29160456e-02,  1.91466003e-02, -1.29063648e-02,\n",
       "          3.57758768e-02, -8.30617852e-03,  7.92039164e-03, -9.37882319e-03,\n",
       "          1.78985301e-03,  1.10326387e-02,  3.88596840e-02,  2.84725644e-02,\n",
       "         -5.57599269e-02,  5.25282479e-03,  3.04193333e-02,  1.18194704e-02,\n",
       "          3.77428421e-04, -2.85049762e-02,  4.84137603e-04,  1.13587074e-02,\n",
       "          1.53522945e-02, -7.13386033e-02, -1.29146388e-02,  5.70328575e-02,\n",
       "          6.11791583e-02, -4.20836932e-03,  3.22605876e-02, -2.58478912e-02,\n",
       "         -5.44861418e-02, -1.01559065e-02,  4.42602144e-04,  3.65939484e-03,\n",
       "          2.02536230e-02,  1.28927833e-02, -4.79020607e-02, -2.13203819e-02,\n",
       "          1.26519155e-02,  1.91846489e-02, -3.56603174e-02, -2.39755062e-02,\n",
       "          2.68558575e-02,  4.45468464e-02,  2.73516585e-02,  2.94502061e-02,\n",
       "         -8.03588236e-04,  2.55145203e-02, -1.00788602e-03,  3.76716545e-03,\n",
       "          2.54919603e-02,  1.30778957e-02,  7.92427243e-02,  1.01099472e-03,\n",
       "         -3.34101476e-03,  2.26034615e-02, -9.79706472e-03, -7.39797039e-03,\n",
       "          2.86032250e-02, -3.39696927e-02, -7.53932932e-03, -5.28486182e-03,\n",
       "         -1.42161058e-02,  1.60060105e-02,  1.67508845e-02,  7.36529254e-03])),\n",
       " (1,\n",
       "  array([-2.37252236e-02,  4.76894551e-04, -3.69032333e-02,  4.08581568e-02,\n",
       "         -2.07472848e-02,  2.37203572e-02,  8.60814180e-03, -3.67772975e-02,\n",
       "          1.40866412e-02,  2.41261646e-02,  6.26059189e-03,  9.93302496e-03,\n",
       "          1.92783896e-03, -5.13435852e-03,  1.83311366e-02, -5.11693989e-02,\n",
       "         -3.96953031e-03,  3.74162302e-02,  3.39993926e-02,  5.13876423e-02,\n",
       "          4.74065877e-03,  3.42571142e-02, -3.00737343e-02, -2.61210815e-02,\n",
       "         -3.69556585e-03, -2.09423281e-02,  6.06851964e-03,  1.03023774e-02,\n",
       "          1.55585592e-02,  3.02604515e-02, -2.72946368e-02,  8.60775434e-02,\n",
       "         -4.80920629e-02,  3.41882533e-02, -2.22644537e-02, -3.11618937e-02,\n",
       "          2.08796114e-03, -3.46911893e-02,  1.32585582e-02,  1.97531235e-03,\n",
       "         -1.07597300e-02, -2.12224698e-04, -1.76907482e-02,  1.86948873e-02,\n",
       "         -7.34325650e-03,  1.46476518e-02,  4.56330220e-03, -2.27126468e-03,\n",
       "          3.15594051e-02,  8.18791845e-03,  1.08046892e-02, -3.55348604e-02,\n",
       "         -1.69194618e-02, -3.42122240e-02, -1.25186119e-02,  4.89567909e-02,\n",
       "          2.10926182e-02,  1.51541791e-02,  2.64172455e-02,  4.63944385e-02,\n",
       "         -1.47019053e-02, -2.59905396e-03,  5.42762884e-02, -1.53303568e-03,\n",
       "         -2.86245896e-02, -3.87183201e-02, -7.21894977e-03,  4.22220532e-02,\n",
       "          1.94513171e-02,  4.36349633e-03,  2.42739024e-02,  1.68686648e-03,\n",
       "          5.63466630e-02,  1.18452764e-03, -1.37912418e-02,  4.49303951e-02,\n",
       "          5.03818460e-02,  3.34552900e-02, -4.44518822e-02,  9.72771905e-03,\n",
       "         -2.13268767e-03, -1.87491887e-02,  1.23077602e-02,  2.28253027e-02,\n",
       "         -3.49813520e-02,  8.99381871e-03, -2.91257661e-02,  2.95900181e-02,\n",
       "          6.67345751e-03, -7.14969824e-03,  1.38440795e-02, -4.79208262e-02,\n",
       "          3.74909961e-02, -2.21828406e-02,  3.23294950e-02,  1.01812146e-02,\n",
       "          8.37160621e-03,  2.71336918e-02, -2.69093829e-02,  1.30459939e-02,\n",
       "         -9.52766902e-03, -2.53324824e-02,  3.92305769e-03, -1.97223457e-02,\n",
       "         -5.02585935e-02, -2.35390871e-02, -4.43366295e-02,  4.62722629e-02,\n",
       "          8.85502056e-03, -1.30483109e-02, -2.64128994e-02, -2.26363764e-02,\n",
       "         -4.24074776e-02, -1.82039415e-02, -9.11536703e-04, -2.45111569e-02,\n",
       "          3.07476501e-02,  1.36698210e-02, -3.07153671e-02,  4.63352039e-02,\n",
       "          5.53913753e-02,  5.20682321e-03, -2.95954665e-03,  3.79152124e-02,\n",
       "          2.56028684e-02, -2.23947112e-04,  2.00923130e-02,  2.82493990e-02,\n",
       "          1.78364330e-02,  3.56606664e-02, -5.09422743e-03,  3.24441318e-02,\n",
       "         -3.93182956e-02, -2.67688254e-02,  1.40252373e-02,  8.10924442e-03,\n",
       "         -3.29617330e-02,  8.98825391e-03,  3.03632976e-02,  3.19177698e-02,\n",
       "          1.53502251e-02,  3.05331326e-02,  3.62079617e-02,  5.84162802e-05,\n",
       "         -3.24697157e-04,  6.17169489e-02, -2.25333878e-02, -4.07757524e-02,\n",
       "         -7.07425765e-03,  1.16139032e-03,  2.53476445e-02, -2.52314547e-02,\n",
       "         -1.93350781e-02,  8.17775187e-03, -4.14071132e-02, -1.08329777e-03,\n",
       "         -1.82844099e-02, -2.30999037e-02,  3.05902483e-02,  9.63274802e-03,\n",
       "          1.00448359e-02,  3.07833958e-02, -4.24130110e-02,  1.27989399e-02,\n",
       "          3.73153370e-02,  9.50714365e-04, -2.29640953e-02, -3.07950970e-02,\n",
       "          6.89051186e-02,  3.19601269e-02, -3.61987624e-02, -5.52540093e-02,\n",
       "         -5.42550449e-02, -1.93214287e-02, -5.17937529e-03,  4.79993892e-02,\n",
       "         -1.62501905e-03,  2.88432708e-02, -7.00836068e-03,  8.46933436e-03,\n",
       "          1.38183663e-02, -4.04851636e-02,  9.27089745e-04,  1.93406267e-02,\n",
       "         -2.87864910e-02,  2.62668010e-02, -1.44142365e-02, -4.68222767e-02,\n",
       "          4.19580439e-02,  8.68781834e-03,  1.60933677e-02, -7.19147927e-02,\n",
       "          8.13104187e-03,  4.84393635e-02, -2.44504686e-02, -2.65521169e-02,\n",
       "         -9.06337031e-03, -5.76072801e-03, -5.63330049e-02, -2.32981213e-02,\n",
       "          1.64918446e-02, -1.99965079e-02,  9.17666404e-03,  1.19428902e-02,\n",
       "          4.59337682e-02, -6.59107395e-02, -6.02180367e-02,  3.83965658e-03,\n",
       "          9.85001094e-03,  1.89676507e-02,  1.19204617e-01,  1.67676263e-02,\n",
       "         -5.45834793e-03, -3.10237662e-02,  5.14916103e-03, -2.53616777e-02,\n",
       "          2.45421747e-03, -4.66020917e-02, -2.59178088e-02,  1.66503994e-02,\n",
       "         -8.15949425e-03, -3.69619979e-02, -2.65582353e-02,  5.79800647e-02,\n",
       "         -2.00045116e-02,  3.99138792e-02, -2.78855500e-02, -4.98976866e-02,\n",
       "          9.15571755e-03, -8.29630731e-03,  2.08469028e-02,  3.93607033e-02,\n",
       "         -1.35193985e-03, -1.88381434e-03, -8.54380919e-03, -1.30353221e-03,\n",
       "          1.45806967e-04,  2.08054315e-02,  2.67910889e-03, -1.64286622e-02,\n",
       "          5.26536538e-03,  6.34017428e-03,  3.19067754e-03,  4.69571908e-02,\n",
       "         -7.71473853e-03,  2.37380770e-02,  3.04084542e-02,  2.86306745e-02,\n",
       "          5.29605910e-05,  4.95967662e-03,  2.81072365e-02,  8.11315833e-03,\n",
       "         -2.59661748e-02, -2.54207211e-02,  2.34273998e-03,  1.89739903e-02,\n",
       "         -2.76694862e-02, -6.06956232e-02,  2.40128913e-02,  2.37988212e-02,\n",
       "          4.29553766e-02, -2.40115775e-03,  3.49087541e-02, -3.99568267e-02,\n",
       "         -5.35673013e-02, -2.73497749e-02, -2.56503884e-02, -2.16127448e-02,\n",
       "         -2.41697432e-02, -4.92117823e-03, -2.25956665e-02, -5.53109286e-03,\n",
       "          3.87661340e-03,  2.79161238e-02, -4.23508217e-02, -6.98332114e-03,\n",
       "          2.85364035e-02,  5.16374233e-02, -9.71595025e-03,  5.11675189e-03,\n",
       "          2.01476264e-02,  4.56422074e-03,  2.40091678e-02,  3.01724466e-02,\n",
       "          3.71054614e-02,  1.60475946e-02,  5.31918278e-02,  3.52448603e-02,\n",
       "         -2.35555854e-02,  3.82453409e-02, -5.50304855e-03,  1.92906959e-02,\n",
       "          8.31276577e-03, -1.84731279e-02,  2.68793952e-03,  1.69061216e-02,\n",
       "          5.69998927e-03,  2.01139677e-02,  4.05087642e-02,  2.45402060e-02]))]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectors_of_lines[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['poetry',\n",
       "  'interrupt',\n",
       "  'ukraine',\n",
       "  'invasion',\n",
       "  'breaking',\n",
       "  'news',\n",
       "  'applebee'],\n",
       " ['literally',\n",
       "  'dancing',\n",
       "  'cowboy',\n",
       "  'care',\n",
       "  'next',\n",
       "  'russia',\n",
       "  'invades',\n",
       "  'ukraine',\n",
       "  'banner',\n",
       "  'unfortunate',\n",
       "  'juxtaposition',\n",
       "  'nightmare',\n",
       "  'advertisers']]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "poems_tokens[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  array([-0.0248435 ,  0.00289008,  0.0081701 ,  0.03408529, -0.02961566,\n",
       "          0.00961857, -0.00237798, -0.01614437,  0.01727663,  0.04069366,\n",
       "          0.04725029,  0.04735925,  0.0110001 , -0.00230699, -0.01501697,\n",
       "         -0.02970825,  0.02158169,  0.02287983,  0.05006909,  0.06047099,\n",
       "         -0.01420604,  0.05101112, -0.04351609, -0.03165736,  0.03028389,\n",
       "         -0.04018218,  0.01470897, -0.02829269,  0.06554529,  0.02702795,\n",
       "         -0.04073085,  0.05149144, -0.03657273,  0.02932359,  0.0144959 ,\n",
       "          0.00773226, -0.03552485,  0.02774676,  0.0525759 , -0.0100171 ,\n",
       "          0.00188385, -0.03376595,  0.04151331,  0.01033899, -0.01032549,\n",
       "         -0.00526202,  0.00134311, -0.0100283 ,  0.00674086, -0.03803946,\n",
       "          0.01386025, -0.01791923,  0.01233035,  0.0050413 , -0.04760404,\n",
       "          0.00211408,  0.02396639,  0.02994179,  0.00221273,  0.02372354,\n",
       "          0.02124269,  0.04442902,  0.03688224,  0.00074273,  0.00260212,\n",
       "         -0.01268472, -0.0308846 ,  0.04455226, -0.03585689,  0.0025848 ,\n",
       "          0.02583414, -0.01579535,  0.06617082,  0.00602579,  0.00837296,\n",
       "          0.02791755,  0.04812037,  0.02635383, -0.00986627, -0.00949149,\n",
       "          0.00394723,  0.06328529, -0.02535174,  0.00815396, -0.02565717,\n",
       "         -0.02702345, -0.00480143,  0.03740098, -0.02624367, -0.00725966,\n",
       "         -0.00561741, -0.02155253,  0.0763166 , -0.05011401,  0.03478337,\n",
       "          0.01080652, -0.01666325,  0.00559024, -0.01435309, -0.00618115,\n",
       "         -0.01361685, -0.03377342,  0.04132476, -0.02282159, -0.02459095,\n",
       "         -0.01874502, -0.03410636,  0.06133406, -0.01698345,  0.02041985,\n",
       "         -0.01499507, -0.01941423, -0.03900588, -0.04376744, -0.00590506,\n",
       "         -0.00985267,  0.02535414,  0.0162918 , -0.02380605,  0.03783637,\n",
       "          0.03860806,  0.02222561, -0.05400686,  0.01862567,  0.01441515,\n",
       "          0.00342722,  0.01050031,  0.00991373,  0.03255906,  0.04637546,\n",
       "          0.01103341,  0.00062354,  0.01540565, -0.06963731,  0.02796283,\n",
       "          0.008365  , -0.04154688, -0.00683506,  0.04358977,  0.03364887,\n",
       "         -0.0092059 ,  0.02615215,  0.06668338,  0.02610903, -0.03046494,\n",
       "          0.05533157,  0.00493696, -0.04449617,  0.00653689, -0.02961369,\n",
       "          0.06666024, -0.05014619, -0.02948405, -0.02063315, -0.01738618,\n",
       "         -0.00014905, -0.01631586, -0.0197754 ,  0.0202643 ,  0.02741299,\n",
       "         -0.01088034,  0.06010522, -0.03442125, -0.0242249 ,  0.06829597,\n",
       "          0.00434523, -0.05085414, -0.02764026,  0.05501587, -0.01706975,\n",
       "         -0.02660472, -0.02299615, -0.07675573, -0.01491282, -0.02320616,\n",
       "          0.03346337,  0.01581072,  0.04963891, -0.00610062, -0.00163229,\n",
       "         -0.01532034, -0.03550596,  0.03703509,  0.0182229 , -0.00864528,\n",
       "          0.06560371,  0.01300236, -0.03632582,  0.02754657,  0.02633774,\n",
       "          0.03531535, -0.07381062,  0.04646761,  0.00354678, -0.04968859,\n",
       "         -0.00256716, -0.00208066, -0.00954587, -0.05876946, -0.01521847,\n",
       "          0.03069764, -0.00499998,  0.02776944,  0.01383106,  0.02175976,\n",
       "         -0.04788646, -0.06309133,  0.01251404,  0.05342436,  0.01568162,\n",
       "          0.11069459,  0.02618006,  0.03293008, -0.0144505 , -0.00375997,\n",
       "          0.01254916, -0.01288508, -0.05932293, -0.04681044,  0.0407042 ,\n",
       "         -0.01550248, -0.01912461, -0.06256958,  0.0374875 , -0.01580171,\n",
       "         -0.00661441, -0.05384624, -0.04491434,  0.0029071 , -0.0004429 ,\n",
       "          0.04440993,  0.04588545,  0.00793439, -0.04532128, -0.00106689,\n",
       "          0.00202691,  0.017572  , -0.01001722, -0.02064224, -0.02384782,\n",
       "          0.03343522,  0.02109978,  0.02604923,  0.00142965,  0.00376585,\n",
       "          0.01893575,  0.03591365,  0.01729966, -0.05056881,  0.00919857,\n",
       "          0.01562351,  0.00429547, -0.02005969, -0.00181192,  0.02110637,\n",
       "          0.01056954, -0.03033893, -0.03305125,  0.02583958,  0.02533704,\n",
       "          0.02809586, -0.01488661,  0.03058053, -0.01467108, -0.03553154,\n",
       "          0.005874  ,  0.01223135, -0.03456106,  0.00820163,  0.01100249,\n",
       "         -0.01147366,  0.01089502, -0.00809357, -0.00177785, -0.00412684,\n",
       "         -0.02990719,  0.02084047,  0.0615309 ,  0.0030361 ,  0.01649527,\n",
       "          0.01594349, -0.00789308, -0.02510462,  0.01260993,  0.04999624,\n",
       "         -0.00678899,  0.03314765,  0.04260818, -0.02896144,  0.03287413,\n",
       "         -0.04771549, -0.01071296,  0.0202921 , -0.03338982, -0.00811313,\n",
       "         -0.01712338, -0.0284597 ,  0.01068155,  0.01082657,  0.04572584])),\n",
       " (1,\n",
       "  array([-1.21983132e-03,  5.36247371e-03, -4.06786740e-02,  8.68131376e-03,\n",
       "          1.30551503e-02,  8.34043491e-04,  1.31308496e-02, -1.25980425e-02,\n",
       "          2.76812669e-02,  2.47445875e-02,  2.44670261e-02,  1.60640620e-02,\n",
       "          8.60665734e-03, -2.82236422e-02, -5.11671480e-02, -1.91426196e-02,\n",
       "         -1.36163862e-02,  1.35705256e-02,  3.02415052e-02,  2.68334925e-02,\n",
       "         -1.41682366e-02,  2.88812400e-02, -6.61857422e-02, -2.24615005e-02,\n",
       "          3.54582425e-03,  2.08819100e-02,  1.71787552e-02,  1.12588159e-02,\n",
       "         -5.43614154e-03,  2.54376392e-02, -2.39902627e-02,  3.56216411e-02,\n",
       "         -1.43909746e-02,  1.64796802e-02,  1.37387403e-02, -7.54166873e-03,\n",
       "         -1.09256714e-02,  1.92414689e-02,  4.50564325e-02,  9.59785903e-03,\n",
       "          2.78988757e-02, -4.59608429e-03,  2.14786460e-02,  2.89583967e-02,\n",
       "         -2.87108768e-03,  3.14714420e-03, -7.50762359e-03, -2.12730130e-02,\n",
       "          3.64907614e-02,  2.43085785e-03, -6.37572676e-04, -2.52793863e-02,\n",
       "         -1.34900973e-02,  1.35428141e-02, -3.79344435e-02, -1.01203572e-02,\n",
       "         -2.02710730e-03, -9.40690187e-03, -1.81432400e-02,  3.89817264e-02,\n",
       "          2.93258065e-02,  1.52014975e-02,  4.67368940e-02,  1.20234310e-02,\n",
       "         -1.00933503e-02, -6.36125446e-03,  1.45778381e-02,  3.95668103e-03,\n",
       "         -2.76881695e-02, -2.50498025e-04, -3.90340712e-03, -1.98358822e-02,\n",
       "          2.74708525e-02,  2.81492964e-02, -9.99595041e-03,  2.32063434e-02,\n",
       "          3.39587010e-02,  5.74172481e-03, -2.10819707e-02, -2.18619253e-02,\n",
       "          3.65909655e-02,  3.30208944e-02, -2.32280736e-02, -3.37469512e-04,\n",
       "         -1.16186678e-02, -1.48747138e-02,  1.19079763e-02,  4.20596337e-02,\n",
       "          6.65651933e-03,  4.04341591e-02,  5.66215228e-03, -2.21217146e-02,\n",
       "          4.52545977e-02, -4.61280955e-02,  3.37022116e-02,  1.97468356e-03,\n",
       "          6.82084942e-03,  3.00423273e-03,  1.26753386e-02, -1.84319538e-02,\n",
       "         -2.47529987e-02, -2.74806993e-02,  4.49340903e-02, -1.36338225e-04,\n",
       "         -2.54674323e-02,  9.01864248e-03, -1.28076584e-02,  4.68630083e-02,\n",
       "         -1.57910748e-02, -5.55402927e-04, -1.61153479e-02, -3.00274954e-02,\n",
       "         -3.08075562e-02, -2.74668239e-02, -1.32603047e-02,  5.55727363e-04,\n",
       "          1.48733902e-02,  1.52684170e-02, -1.14144095e-02,  1.82624641e-02,\n",
       "          3.30360076e-02, -7.39428104e-03, -2.58430513e-02,  1.44389538e-02,\n",
       "         -1.51032538e-02,  1.18682405e-02,  1.74771496e-02,  7.04515213e-03,\n",
       "          8.34492243e-03,  3.88289475e-02,  3.20085524e-02,  9.02301794e-04,\n",
       "         -1.07586310e-02, -1.64570648e-03,  3.02092593e-02, -2.73512002e-03,\n",
       "         -2.65721374e-03,  2.83338994e-03,  1.84237553e-04,  1.79300066e-02,\n",
       "          1.66109142e-03,  2.10561420e-02,  6.21405857e-02,  3.57220102e-03,\n",
       "          1.94764468e-02,  5.03219418e-02, -3.72207993e-02, -1.21547202e-02,\n",
       "         -3.74952628e-03, -1.50729098e-02,  2.16606655e-02, -2.64555510e-02,\n",
       "         -2.31679897e-02,  1.67366383e-02, -3.03451060e-02, -2.64778755e-03,\n",
       "         -2.28677742e-02, -2.30066799e-02,  2.44626029e-02, -1.77995150e-02,\n",
       "         -1.67139173e-02,  3.48722235e-02, -3.13674806e-03, -2.70747337e-02,\n",
       "          5.01856854e-02, -2.96731243e-02, -3.77391128e-02, -2.44731552e-03,\n",
       "          4.51255827e-02,  2.11356638e-02, -2.19743535e-02, -2.20983112e-02,\n",
       "         -2.13680250e-02, -2.50932944e-03, -1.78870200e-02,  3.08122449e-02,\n",
       "          1.74127649e-02,  2.34779138e-02, -3.38825407e-03,  1.84200632e-02,\n",
       "          8.76730384e-03,  1.14412875e-02, -9.29856776e-03,  1.24917108e-02,\n",
       "         -4.25383466e-03,  5.83262114e-02, -7.76273238e-03, -1.96370510e-02,\n",
       "          4.98577557e-02,  1.14694212e-04,  3.07172655e-02, -4.74901101e-02,\n",
       "          2.70504683e-02,  3.69219340e-02, -2.02623715e-02, -1.66668861e-02,\n",
       "         -2.15779423e-02,  1.12950088e-02, -3.79557678e-02, -3.00693097e-02,\n",
       "          9.26286877e-05, -5.92378317e-02,  4.22467831e-03, -6.04727965e-03,\n",
       "          1.05221540e-02, -2.65575784e-02, -1.45720964e-02, -7.59565859e-03,\n",
       "          1.50966045e-02,  4.29038807e-02,  7.25422592e-02,  5.05872551e-02,\n",
       "          2.73874291e-02, -5.18773781e-03, -1.76048357e-02, -7.01339741e-03,\n",
       "         -5.89836990e-03, -2.38153110e-02, -4.25641973e-02,  1.45058421e-02,\n",
       "         -7.88031098e-03, -2.19234011e-02, -7.07578181e-02,  4.54144942e-02,\n",
       "         -3.69927307e-02, -1.52937982e-04, -2.53410747e-02, -3.47572673e-02,\n",
       "          4.33874327e-02,  8.35336232e-03,  7.72265434e-03,  4.12896100e-02,\n",
       "          5.50994544e-03, -1.01420668e-02, -2.69019380e-03,  2.33340583e-02,\n",
       "          1.53137604e-02, -1.19377789e-02,  1.74428281e-03, -2.33401634e-02,\n",
       "          9.82277760e-03, -1.70155765e-02,  1.06402108e-02,  2.34825957e-02,\n",
       "          8.64923992e-03,  3.37927574e-02,  5.02626812e-02, -6.10088720e-03,\n",
       "          2.22817939e-02,  9.00792491e-04,  1.77604260e-02, -1.87941226e-02,\n",
       "         -1.46220189e-02, -1.16454473e-02,  1.29622911e-02,  1.65680659e-02,\n",
       "         -2.23704188e-03, -6.72033783e-03,  7.89259845e-03,  2.64616472e-03,\n",
       "         -2.88587705e-04, -1.24280572e-03,  1.68733137e-02, -1.74170054e-02,\n",
       "         -3.90641742e-02, -1.82990386e-02,  1.34314646e-02,  2.95687604e-04,\n",
       "          1.03607362e-02, -1.50730261e-02, -6.60267770e-03,  1.80208833e-03,\n",
       "          6.94028247e-03, -8.26332144e-03,  1.80103016e-02,  1.78499579e-02,\n",
       "          9.34328511e-03,  3.90537999e-02,  1.59042123e-02,  2.60579822e-05,\n",
       "          2.90007504e-03,  3.03238477e-02, -7.68817803e-03,  1.27862744e-02,\n",
       "          3.20251098e-02,  4.07448508e-03, -2.99289285e-02,  2.85710010e-03,\n",
       "          9.17998918e-04,  3.03365929e-02, -4.22727725e-02, -1.40872540e-02,\n",
       "          1.44969245e-02,  4.10990732e-03, -3.52008479e-02, -4.76355885e-03,\n",
       "         -2.72754673e-04,  9.89896119e-03,  2.99266626e-02,  2.88857527e-02])),\n",
       " (2,\n",
       "  array([-3.97190610e-02, -5.69591943e-03, -2.45317860e-02,  2.63025267e-02,\n",
       "          2.07239445e-03, -3.51665002e-03,  2.08714440e-02, -1.37534691e-02,\n",
       "         -5.30821987e-03,  3.88792555e-02,  3.41649436e-02,  2.58648565e-02,\n",
       "          2.04669666e-02, -3.49251891e-02, -5.18213311e-02, -2.43836387e-02,\n",
       "         -1.58228665e-02,  2.78506051e-02,  2.44867667e-02,  7.46824218e-02,\n",
       "          2.97717757e-03,  5.17857684e-02, -8.28096977e-02, -3.09720367e-02,\n",
       "         -1.35699100e-02, -3.82914466e-02,  2.43923125e-02,  2.59237154e-02,\n",
       "          4.63111754e-02,  4.15164929e-02, -5.13861496e-02,  3.27552807e-02,\n",
       "         -5.75073109e-02, -4.76758904e-05,  3.60196631e-02, -3.51352182e-02,\n",
       "         -5.91661866e-02, -7.79238426e-03,  8.62581539e-02,  1.22889761e-02,\n",
       "         -9.87902483e-03, -2.16811604e-02,  5.67809181e-03,  2.49397374e-02,\n",
       "          1.91473020e-02, -2.81217348e-02, -2.91283069e-03, -7.80883342e-03,\n",
       "          3.22082305e-02,  1.83740964e-02,  2.40460379e-03, -4.17771565e-02,\n",
       "         -3.82064928e-02,  6.74953568e-03, -1.31975323e-02, -3.05037343e-04,\n",
       "          2.71933675e-02,  1.70354557e-02, -2.00872405e-02,  7.74368842e-02,\n",
       "          9.65982266e-03,  2.82777231e-02,  1.77540310e-02, -3.19853070e-02,\n",
       "         -3.50089429e-02, -2.43335526e-02, -7.88432019e-03,  3.18060938e-02,\n",
       "         -1.48078569e-02, -2.68502636e-02,  5.78427755e-02, -1.69881909e-02,\n",
       "          2.27780242e-02,  2.86130483e-02, -4.14480839e-03,  6.27837007e-02,\n",
       "          5.83698727e-02,  3.09535774e-02, -6.03673643e-02, -6.70806516e-03,\n",
       "          2.12880570e-02,  3.37640639e-02, -3.50271010e-03,  9.13342701e-04,\n",
       "         -1.68451539e-02, -8.52349503e-04, -3.60480871e-02,  7.97976222e-03,\n",
       "          2.30293223e-02,  4.68587608e-03,  2.38949420e-02, -1.02404872e-01,\n",
       "          2.60218879e-02, -1.25115991e-02,  3.68878936e-02,  1.17648995e-02,\n",
       "          7.69257071e-03, -2.03309600e-02, -1.59291115e-02, -3.06308577e-02,\n",
       "          6.87565085e-03, -3.25034554e-02,  2.92885932e-02, -2.72704137e-02,\n",
       "         -3.91788513e-02,  4.52413701e-02, -2.35751238e-02,  2.89916289e-02,\n",
       "          1.19887562e-02, -8.31557573e-03, -4.74477751e-02, -7.20115417e-03,\n",
       "         -4.04745843e-02, -3.97103675e-02, -2.25243242e-02,  1.04336855e-02,\n",
       "          1.46118537e-02,  3.82624510e-02, -1.82270710e-02,  2.36038430e-02,\n",
       "         -2.91155764e-02, -1.81680667e-02, -2.27511499e-02,  2.55649615e-02,\n",
       "         -4.37838043e-03,  2.27210814e-02, -1.14822595e-02,  3.44774841e-02,\n",
       "          3.29076799e-02,  1.89445171e-02, -7.00216046e-04,  3.11128766e-02,\n",
       "          4.01272215e-02, -1.36598096e-02,  8.97830075e-03,  1.96850648e-02,\n",
       "         -2.13932676e-02, -1.47776225e-02,  1.98435121e-02,  3.51806835e-02,\n",
       "          1.54286848e-02,  1.74021410e-04,  5.42556721e-02,  3.33645257e-02,\n",
       "         -3.16880458e-02,  6.97070200e-02, -2.48688314e-02, -3.69152017e-02,\n",
       "          9.70031992e-03, -4.14334137e-02,  8.03042082e-02, -5.26098737e-02,\n",
       "         -5.53685581e-02, -4.90018050e-03, -2.95096000e-02,  1.67323260e-02,\n",
       "         -1.90494494e-02, -3.02762246e-02,  2.36226910e-02,  1.71136958e-02,\n",
       "         -2.84610956e-03,  6.94920535e-02, -4.86996850e-02,  1.47216414e-02,\n",
       "          4.22341795e-02,  5.81826160e-02, -1.72487218e-02, -1.07442520e-02,\n",
       "          6.06317615e-02, -3.07479002e-03, -4.25776907e-02, -2.11055661e-02,\n",
       "         -4.38717050e-02,  6.60342215e-03, -1.07100080e-02,  2.37976593e-02,\n",
       "          1.50342424e-02,  4.03179265e-02,  1.80738897e-02, -3.71290511e-02,\n",
       "          8.96817333e-03, -3.28975996e-02, -2.61750169e-02,  5.53492119e-02,\n",
       "         -2.22264707e-03,  3.66687243e-02, -1.06723849e-03, -3.06900129e-02,\n",
       "          4.86473333e-02,  9.32835331e-03,  9.52469056e-03, -8.74790220e-02,\n",
       "         -2.10631149e-02,  5.14710958e-02, -5.31723143e-02,  1.17150798e-02,\n",
       "         -3.55194717e-02,  4.89905002e-03, -2.94678132e-02, -6.25993307e-04,\n",
       "          2.55606995e-02, -4.08914220e-02, -4.09652381e-03,  4.89341318e-03,\n",
       "          5.60914083e-02, -9.89339103e-02, -4.35374284e-02, -3.62864446e-03,\n",
       "          2.27545783e-03,  6.27334486e-02,  9.82056532e-02,  3.80812949e-02,\n",
       "         -1.79032486e-02, -3.49112533e-04,  3.62079333e-02,  4.33959345e-03,\n",
       "          8.07049556e-03, -5.32027868e-02, -6.01433263e-02,  6.58398515e-02,\n",
       "          1.93582903e-02, -3.41631075e-02, -6.68713024e-02,  4.71311889e-02,\n",
       "          3.12277056e-02,  4.59246136e-03, -3.70531089e-02, -7.49380502e-02,\n",
       "          3.40359704e-02, -1.27821223e-02,  6.60030996e-02,  4.43182620e-02,\n",
       "          9.13846908e-06, -2.91466374e-03,  1.51433866e-02,  2.13589583e-02,\n",
       "          4.83658092e-02, -1.11741515e-02, -1.30559396e-03, -6.53751827e-03,\n",
       "          2.66432485e-02,  7.01262306e-03,  2.69725335e-02,  4.20946066e-02,\n",
       "         -3.08731559e-02, -9.55726347e-04,  6.46066782e-02, -1.06027210e-02,\n",
       "         -1.49273997e-02,  7.60397450e-03, -1.85923176e-03,  1.35766866e-03,\n",
       "         -6.27528452e-03,  3.28604992e-03,  1.79468952e-02,  3.20950757e-02,\n",
       "         -9.70785043e-03, -3.93944032e-02,  4.43089537e-02,  1.88105626e-02,\n",
       "          4.62419574e-02,  1.86500137e-02,  2.10749371e-02, -1.78035166e-02,\n",
       "         -2.84543710e-02, -1.07467608e-02,  7.28141341e-02, -2.01271023e-02,\n",
       "         -1.60489489e-02, -6.26825604e-03,  4.11046013e-02, -6.55442296e-03,\n",
       "          4.62705407e-02, -3.90535391e-02,  1.22924080e-02, -2.80436996e-02,\n",
       "          3.24429235e-02,  2.68586698e-02,  4.28776791e-02,  3.73062581e-02,\n",
       "          1.42629973e-02,  7.65077200e-03, -1.99218796e-02,  2.40919388e-02,\n",
       "          1.26137280e-02,  4.35045718e-02,  4.04092091e-02, -2.71476682e-02,\n",
       "         -2.93711741e-02,  1.34633005e-02, -6.51458938e-02,  2.75724808e-02,\n",
       "          3.09639992e-02, -2.09208973e-02,  3.44660405e-02, -3.80059168e-02,\n",
       "         -1.40020173e-02,  9.68785551e-03, -1.93348536e-02,  5.64536644e-02]))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectors_of_lines[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "poem_meter = [0] * len(vectors_of_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(vectors_of_lines)):\n",
    "    poem_meter[i] = []\n",
    "    #if i % 2 == 0:\n",
    "    for j in range(len(vectors_of_lines[i])):\n",
    "            vect1 = []\n",
    "            vect1.append(vectors_of_lines[i][j][1])\n",
    "    poem_meter[i] = sum(vect1) / len(vectors_of_lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-3f945bc109a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimilarity_matrix_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoem_meter\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpoem_meter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "#similarity_matrix_1 = np.matrix((poem_meter * poem_meter.T).A, dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "meters = np.asmatrix(poem_meter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_matrix_1 = np.matrix((meters * meters.T).A, dtype=dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "similarity_matrix_1[range(len(filelabels)), range(len(filelabels))] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim_1 = np.array(similarity_matrix_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(filelabels)):\n",
    "    for j in range(len(filelabels)):\n",
    "        SAM[i + len(filelabels)][j + len(filelabels)] = sim_1[i][j][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "G1 = nx.from_numpy_matrix(similarity_matrix_1)\n",
    "\n",
    "weights_1 = [(G1[tpl[0]][tpl[1]]['correlation']) for tpl in G1.edges()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "e = [(x, x) for x in G1.nodes()] \n",
    "\n",
    "G1.remove_edges_from(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "number_of_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coupling_strength = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAM = np.asmatrix(SAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for node in range(len(filelabels)):\n",
    "    for layer1 in range(number_of_layers):\n",
    "        for layer2 in range(number_of_layers):\n",
    "            if layer1 != layer2:\n",
    "                SAM[node + len(filelabels)*layer1, node + len(filelabels)*layer2] = coupling_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "S = np.sum(SAM, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d = np.zeros(2*len(filelabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2*len(filelabels)):\n",
    "    d[i] = 1/S[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = np.diag([d[i] for i in range(2*len(filelabels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.dot(SAM, D) \n",
    "# T is the transition matrix; it is a left stochastic matrix (i.e., all entries are non-negative and each column sums to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "N = 2*len(filelabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = number_of_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = len(filelabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "T=np.dot(SAM,D) # Refresh matrix T\n",
    "r=0.85 # (1 r) is the teleportation rate \n",
    "R=r*T+(1-r)/N*np.ones((N, N))\n",
    "RLeigenvector=np.asmatrix(np.ones(N))\n",
    "RLeigenvector=RLeigenvector.transpose() # Now RLeigenvector is a N*1 column vector\n",
    "while la.norm(RLeigenvector - np.dot(R,RLeigenvector))>10** - 8: \n",
    "    RLeigenvector = np.dot(R,RLeigenvector) / la.norm(np.dot(R,RLeigenvector))\n",
    "NRLeigenvector=RLeigenvector/np.sum(RLeigenvector) \n",
    "NRLeigenvector=NRLeigenvector.reshape((L,K))\n",
    "rankPageRank=np.squeeze(np.asarray(np.sum(NRLeigenvector, axis=0))) \n",
    "print(\"PageRank done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(rankPageRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PageRank = [(filelabels[i], rankPageRank[i]) for i in range(len(filelabels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Sorted_PageRank = sorted(PageRank, key = lambda t: t[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(Sorted_PageRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cos_sim(x,y):\n",
    "    return dot(x, y)/(norm(x)*norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PageRank_1 = [(i, filelabels[i], rankPageRank[i]) for i in range(len(filelabels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Sorted_PageRank_1 = sorted(PageRank_1, key = lambda t: t[2], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(Sorted_PageRank_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "Poem = []\n",
    "Selected_lines = []\n",
    "Cosine = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for f in Sorted_PageRank_1:\n",
    "    v1 = []\n",
    "    for j in vectors_of_lines[f[0]]:\n",
    "        v1.append((j[0], cos_sim(poem_meter[36], j[1])))\n",
    "    Cosine.append(v1)\n",
    "    V = []\n",
    "    V = sorted(v1, key = lambda t: t[1], reverse = True)\n",
    "    Selected_lines.append((f[0], f[1], V[0][0]))\n",
    "    i = V[0][0]\n",
    "    Poem.append(lines[f[0]][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Mirror_Poem = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(filelabels)):\n",
    "    v2 = []\n",
    "    for j in vectors_of_lines[i]:\n",
    "        v2.append((j[0], cos_sim(poem_meter[36], j[1])))\n",
    "    S = sorted(v2, key = lambda t: t[1], reverse = True)\n",
    "    if len(S) >= 2:\n",
    "        j = S[1][0]\n",
    "    else:\n",
    "        j = S[0][0]\n",
    "    Mirror_Poem.append(lines[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a poem says, “no, no. you have feelings. you have fears. you / have questions. let’s get back to the voice and the vocabulary of being human.” \t  annalynne mccord has recited a poem to president putin\n",
      "celebrities go full cringe responding to russia's invasion of ukraine \t  his 'crime' – the poem 'to ukrainian patriots' in which he expresses / his opposition to russia's annexation of crimea and suggests ukrainians\n",
      "russian poet sentenced over poem in support of ukraine \t  russia's war on ukraine, in context | the new yorker\n",
      "he armed conflict in the east of ukraine brought about an emergence of a distinctive trend in contemporary ukrainian lit, the poetry of war \t  biden vows putin, russia will 'bear the consequences' after ukraine invasion\n",
      "tributes flow for ' giant, no nonsense ' feminist author, educator, activist and poet bell hooks \t  khersonska also translates english-language poets into russian, including\n",
      "myroslav laiuk is a ukrainian novelist, dramatist, ... many of whom were actually russian citizens who came to ukraine from russia \t  the early poetry of taras shevchenko, the outstanding ukrainian poet of the 19th ...\n",
      "news brief: russia invades ukraine, biden's reaction, ... \t  olivia wilde reposts poem about ukraine invasion, gets dragged\n",
      "tensions between ukraine and russia have been simmering since 2014. ... / descriptions of the lives of ordinary people, and even poetry \t  taras shevchenko - wikipedia\n",
      "the literally dancing cowboy that don't care next to the russia invades ukraine banner / is an unfortunate juxtaposition nightmare for advertisers \t  lesson of the day: 'the invasion of ukraine: how russia attacked ...\n",
      "sophie pinkham on poetry vs propaganda in ukraine ... new poems from ukraine ... and registers – that constitute contemporary russia's worship of war \t  in pavlo tychyna's famous cycle of poems instead of sonnets and octaves, ...\n",
      "the latest news on putin's invasion of ukraine, the conditions for / civilians on the ground, and what the crisis means for the u.s. and \t  transreading russian and ukrainian poems of conflict\n",
      "putin: how can this heritage be divided between russia and ukraine? /... i should add that works of fiction, books of ukrainian poetry and folk \t  decomposition of words - tls - times literary supplement\n",
      "nadim's poem about taking off \"your brave feeling\" after school has won him ... 'most severe sanctions ever' on russia over ukraine invasion \t  calamity again - the atlantic\n",
      "poetry on fire: a personal journey through ukraine’s executed renaissance \t  [poetry] we interrupt the ukraine invasion for breaking news from... applebee's\n",
      "zakharova responded to fakes about the “invasion” of the russian federation into ukraine with tyutchev's poems \t  article by vladimir putin ”on the historical unity of russians and ...\n",
      "olivia wilde is among the celebrities who are talking about the ukraine invasion, but she got dragged on twitter for it \t  zakharova responded to fakes about the “invasion” of the russian ...\n",
      "ukraine feature: words for war - poetry international online \t  words for war: new poems from ukraine - academic studies press\n",
      "scott morrison has branded russian president vladmir putin a \"thug\" \t  (pdf) argumentation and aggression: about maps and poems in ...\n",
      "will the world keep watching the gunfire / where innocent people will keep dying \t  ukrainian-british poet and translator stephen komarnyckyj reminds that russia’s soviet / hegemony over ukraine was made possible by stalin’s massacre of ukrainian elites\n",
      "in poetry and in multimodal forms like political maps / ... ukraine and russia provided a simple reason to vote for russia \t  myroslav laiuk (poet) - ukraine - poetry international\n",
      "as early as the winter of 1940, some 18 months before the nazi invasion / ... pinn's poems about the splendid landscape and nature in the ukraine \t  named by bbc as \"one of 12 artists that changed the world\", ilya kaminsky a ukrainian-russian-jewish-american poet\n",
      "russian poet faces new criminal charges for poem in support of ... \t  songs of ukraina, with ruthenian poems. by florence randal ...\n",
      "in this feature, elżbieta wójcik-leese presents five poems written as part of 'transreading russia' \t  putin justifies ukraine invasion as a 'special military operation'\n",
      "if i was your mother, the world would have been warm / so much laughter and joy and nothing would harm \t  young jewish poets who fell as soviet soldiers in the second world war\n",
      "ukraine needs the world's love, help, and prayers now \t  historical poems - poetry x hunger... holodomor\n",
      "who is ready to fight alongside us? i don't see anyone. who is ready to give / ukraine a guarantee of nato membership? everyone is afraid \t  ukraine to surrender? ready to discuss neutrality\n",
      "after the mongol invasion (13th century), ukrainian literature was in ... \t  alexander byvshev is facing another prosecution under russia's 'anti-extremism' legislation over a poem entitled 'on ukraine's independence'\n",
      "catherine the great was praised in verse not just by domestic poets / such as sumarokov and derzhavin, but also by her long-time correspondent and friend ... \t  love in kyiv written by natalka bilotserkivets and translated by andrew sorokowsky\n",
      "hear me out, dear lord / for i don't know what i've done / i have stolen not a penny / nor have i been a bad son. \t  \"aggressive and unlawful\" invasion of ukraine\n",
      "more terrible is love in kyiv than / magnificent venetian passions \t  elon musk quoted an ancient chinese poem and twitter can’t decide\n",
      "taras hryhorovych shevchenko also known as kobzar taras, or simply kobzar \t  tributes flow for ' giant, no nonsense ' feminist author, educator, activist and poet bell hooks\n",
      "the poem is called “calamity again.” the original version was written in ukrainian, in 1859 \t  a poem says, “no, no. you have feelings. you have fears. you / have questions. let’s get back to the voice and the vocabulary of being human.”\n",
      "song of departure: a bride of bukovina / the mother speaks / pathway, little garden / (ah, she must depart!) / when i gaze upon you / faints my breaking heart \t  ukraine - russia - give peace a chance poem by bernedita rosinha\n",
      "give me your tired and your poor who can stand on their own two feet and who will not become a public charge \t  'take off your brave': 5-year-old twitter poet has a book out\n",
      "central to the collection is the \"what is silence?\" / it opens in an occupied country in a time of political unrest \t  rus - ukraine - russia: scenes from the cultural history\n",
      "beanstalks are ignited to boil beans / the beans in the pot cry out / we are born of the same root / why should we incinerate each other \t  official rewrites emma lazarus 's statue of liberty poem: give me your tired, your poor...\n",
      "[antwerp] twerp am i crawling indoor cafes away from the calculated sun \t  antiwar demo ex cathedra markt the frozen crowd an online ant\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(Poem)):\n",
    "    print(Poem[i], '\\t ', Mirror_Poem[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
